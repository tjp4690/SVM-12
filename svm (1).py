# -*- coding: utf-8 -*-
"""SVM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wCCkKhq-19kmuqKZfgxaaCymJHpledDE

## Question 1: What is a Support Vector Machine (SVM), and how does it work?


 Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks. However, it is mostly used for classification.

⚙️ How SVM Works:
Linear SVM:

It finds the best decision boundary (hyperplane) that separates the data into classes.

The best hyperplane is the one that maximizes the margin between the two classes.

The data points that are closest to the hyperplane are called support vectors.

Non-Linear SVM:

When data is not linearly separable, SVM uses a technique called the kernel trick to transform the data into a higher-dimensional space where it becomes linearly separable.

# Question 2: Explain the difference between Hard Margin and Soft Margin SVM.

| Feature            | Hard Margin SVM              | Soft Margin SVM         |
| ------------------ | ---------------------------- | ----------------------- |
| Data Type          | Perfectly linearly separable | Not perfectly separable |
| Tolerance to Error | No                           | Yes (controlled by `C`) |
| Robust to Noise    | ❌ No                         | ✅ Yes                   |
| Flexibility        | ❌ Rigid                      | ✅ Flexible              |
| Usage in Practice  | Rarely used                  | Commonly used           |

# Question 3: What is the Kernel Trick in SVM? Give one example of a kernel and explain its use case.

The Kernel Trick is a mathematical technique used in SVM to solve non-linearly separable problems by implicitly mapping the data into a higher-dimensional space, without actually computing the coordinates in that space.

This allows SVM to create a linear separator in the transformed (higher-dimensional) space, which corresponds to a nonlinear separator in the original space.

Example Kernel: Radial Basis Function (RBF) Kernel

📘 Formula:
𝐾
(
𝑥
,
𝑥
′
)
=
exp
⁡
(
−
𝛾
∥
𝑥
−
𝑥
′
∥
2
)
K(x,x
′
 )=exp(−γ∥x−x
′
 ∥
2
 )

🛠 Use Case:

Image classification, text classification, or bioinformatics, where:

Classes are not linearly separable

You want to model complex, non-linear boundaries

💡 Behavior:
The RBF kernel measures similarity between two data points.

It maps the data into infinite-dimensional space.

The parameter γ controls how far the influence of a single training example reaches.

# Question 4: What is a Naïve Bayes Classifier, and why is it called “naive”?

The Naïve Bayes Classifier is a supervised machine learning algorithm based on Bayes’ Theorem with a strong (naïve) assumption that all features are independent of each other, given the class.

Despite this unrealistic assumption, it works surprisingly well in many practical applications, especially in text classification.

🧠 Why is it called "Naïve"?

It is called naïve because it assumes that all input features are independent of each other — an assumption that is rarely true in real-world data.

Example:
In spam detection, the words "free" and "money" often appear together — they are not independent, but Naïve Bayes assumes they are.

# Question 5: Describe the Gaussian, Multinomial, and Bernoulli Naive Bayes variants.When would you use each one?

1. 🧮 Gaussian Naïve Bayes
📘 Used When:
Features are continuous (i.e., real numbers like height, weight, income).

Assumes features follow a normal (Gaussian) distribution.

🔬 How It Works:
Calculates the likelihood of features using the probability density function of a Gaussian (bell curve).

📊 Example Use Cases:
Iris dataset (petal/sepal length/width)

Medical diagnosis (body temperature, blood pressure)

✅ Use When:
You have numerical/continuous data.

2. 🔢 Multinomial Naïve Bayes
📘 Used When:
Features are discrete counts (e.g., word counts in a document).

🔬 How It Works:
Uses a multinomial distribution to model the number of times events occur.

📊 Example Use Cases:
Text classification (spam detection, sentiment analysis)

Document/topic classification

Word frequency analysis in NLP

✅ Use When:
You have text data represented as word counts or term frequency.

3. 🟩 Bernoulli Naïve Bayes
📘 Used When:
Features are binary/Boolean (0 or 1, i.e., presence or absence of a feature).

🔬 How It Works:
Each feature is modeled using a Bernoulli distribution.

Looks at whether a word exists or not in a document (not how many times).

📊 Example Use Cases:
Email spam detection using presence of keywords

Binary feature-based sentiment classification

✅ Use When:
You have binary input features (e.g., “does this word appear?”)

**Gaussian Naïve Bayes **
"""

# 1. Import libraries
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import pandas as pd

# 2. Load Iris dataset
iris = load_iris()
X = iris.data  # feature matrix
y = iris.target  # labels (0 = setosa, 1 = versicolor, 2 = virginica)

# 3. Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 4. Train Gaussian Naïve Bayes model
model = GaussianNB()
model.fit(X_train, y_train)

# 5. Predict
y_pred = model.predict(X_test)

# 6. Evaluation
print("✅ Accuracy:", accuracy_score(y_test, y_pred))
print("\n📊 Classification Report:\n", classification_report(y_test, y_pred, target_names=iris.target_names))
print("\n🧩 Confusion Matrix:\n", pd.DataFrame(confusion_matrix(y_test, y_pred),
                                              index=iris.target_names,
                                              columns=iris.target_names))

"""****

Multinomial Naïve Bayes
"""

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, accuracy_score
import numpy as np
import pandas as pd

# Load data
iris = load_iris()
X = iris.data
y = iris.target

# Discretize continuous features into bins
X_discrete = np.floor(X).astype(int)  # Simple binning by flooring values

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_discrete, y, test_size=0.2, random_state=42)

# Train Multinomial NB
model = MultinomialNB()
model.fit(X_train, y_train)

# Predict and evaluate
y_pred = model.predict(X_test)
print("✅ Accuracy:", accuracy_score(y_test, y_pred))
print("\n📊 Classification Report:\n", classification_report(y_test, y_pred, target_names=iris.target_names))

"""** Bernoulli Naïve Bayes **"""

from sklearn.naive_bayes import BernoulliNB

# Binarize the features (e.g., value > mean becomes 1, else 0)
X_binary = (X > X.mean(axis=0)).astype(int)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_binary, y, test_size=0.2, random_state=42)

# Train Bernoulli NB
model = BernoulliNB()
model.fit(X_train, y_train)

# Predict and evaluate
y_pred = model.predict(X_test)
print("✅ Accuracy:", accuracy_score(y_test, y_pred))
print("\n📊 Classification Report:\n", classification_report(y_test, y_pred, target_names=iris.target_names))

"""# **Question 6: Write a Python program to:**

# ● Load the Iris dataset

# ● Train an SVM Classifier with a linear kernel

# ● Print the model's accuracy and support vectors.


"""

# Import required libraries
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Step 1: Load the Iris dataset
iris = datasets.load_iris()
X = iris.data           # Feature matrix
y = iris.target         # Labels

# Step 2: Split into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 3: Create and train SVM with linear kernel
svm_model = SVC(kernel='linear')
svm_model.fit(X_train, y_train)

# Step 4: Make predictions
y_pred = svm_model.predict(X_test)

# Step 5: Evaluate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("✅ Model Accuracy:", accuracy)

# Step 6: Print support vectors
print("\n📌 Number of Support Vectors for each class:", svm_model.n_support_)
print("📌 Support Vectors (first few):\n", svm_model.support_vectors_[:5])

"""# Question 7: Write a Python program to:

# ● Load the Breast Cancer dataset

# ● Train a Gaussian Naïve Bayes model

# ● Print its classification report including precision, recall, and F1-score.

"""

# Step 1: Import necessary libraries
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import classification_report

# Step 2: Load the Breast Cancer dataset
data = load_breast_cancer()
X = data.data      # Feature matrix
y = data.target    # Labels (0 = malignant, 1 = benign)

# Step 3: Split data into training and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 4: Train Gaussian Naïve Bayes classifier
model = GaussianNB()
model.fit(X_train, y_train)

# Step 5: Make predictions
y_pred = model.predict(X_test)

# Step 6: Print classification report
print("📊 Classification Report:")
print(classification_report(y_test, y_pred, target_names=data.target_names))

"""# Question 8: Write a Python program to:

# ● Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best C and gamma.

# ● Print the best hyperparameters and accuracy.




"""

# Step 1: Import required libraries
from sklearn.datasets import load_wine
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Step 2: Load the Wine dataset
wine = load_wine()
X = wine.data
y = wine.target

# Step 3: Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 4: Define SVM model and parameter grid
model = SVC()
param_grid = {
    'C': [0.1, 1, 10, 100],
    'gamma': [0.001, 0.01, 0.1, 1],
    'kernel': ['rbf']
}

# Step 5: Grid Search with 5-fold cross-validation
grid = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')
grid.fit(X_train, y_train)

# Step 6: Print best parameters and accuracy on test set
best_model = grid.best_estimator_
y_pred = best_model.predict(X_test)

print("✅ Best Hyperparameters:", grid.best_params_)
print("🎯 Accuracy on Test Set:", accuracy_score(y_test, y_pred))

"""# Question 9: Write a Python program to:

# ● Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using sklearn.datasets.fetch_20newsgroups).

# ● Print the model's ROC-AUC score for its predictions.
"""

# Step 1: Import libraries
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score

# Step 2: Load subset of newsgroups (binary classification for ROC-AUC)
categories = ['rec.sport.baseball', 'sci.med']  # Two categories only
data = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))

# Step 3: Vectorize text data
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(data.data)
y = data.target

# Step 4: Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 5: Train Multinomial Naive Bayes model
model = MultinomialNB()
model.fit(X_train, y_train)

# Step 6: Predict probabilities and compute ROC-AUC score
y_prob = model.predict_proba(X_test)[:, 1]  # Probabilities for class 1
auc_score = roc_auc_score(y_test, y_prob)

print("🎯 ROC-AUC Score:", auc_score)

"""# Question 10: Imagine you’re working as a data scientist for a company that handles email communications.

# Your task is to automatically classify emails as Spam or Not Spam. The emails may
# contain:

# ● Text with diverse vocabulary

# ● Potential class imbalance (far more legitimate emails than spam)

# ● Some incomplete or missing data

# Explain the approach you would take to:

# ● Preprocess the data (e.g. text vectorization, handling missing data)

# ● Choose and justify an appropriate model (SVM vs. Naïve Bayes)

# ● Address class imbalance

# ● Evaluate the performance of your solution with suitable metricsAnd explain the business impact of your solution.


"""

# Import necessary libraries
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, roc_auc_score
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns

# Load binary classification dataset (Spam-like: 'talk.politics.misc' vs Ham-like: 'sci.space')
categories = ['talk.politics.misc', 'sci.space']
data = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))

# Check for class imbalance
print("Class Distribution:", Counter(data.target))

# Handle missing values (replace with empty string)
texts = [text if text is not None else "" for text in data.data]

# Text preprocessing & TF-IDF vectorization
vectorizer = TfidfVectorizer(stop_words='english', lowercase=True, max_features=5000)
X = vectorizer.fit_transform(texts)
y = data.target

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Multinomial Naive Bayes
model = MultinomialNB()
model.fit(X_train, y_train)

# Predict labels and probabilities
y_pred = model.predict(X_test)
y_proba = model.predict_proba(X_test)[:, 1]  # Prob for class 1

# Print evaluation metrics
print("\n--- Classification Report ---")
print(classification_report(y_test, y_pred))

# ROC-AUC Score
roc_auc = roc_auc_score(y_test, y_proba)
print("🎯 ROC-AUC Score:", roc_auc)

# Optional: Heatmap of confusion matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)

sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=data.target_names, yticklabels=data.target_names)
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()